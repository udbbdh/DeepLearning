{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4032dc09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torchviz import make_dot\n",
    "\n",
    "\n",
    "x = np.random.rand(100, 1)\n",
    "y = np.random.rand(100, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "92e3e288",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对输入数据的处理\n",
    "# 1 将数据从 numpy 类型转换为 tensor 类型, float()是强制转化为float类型\n",
    "# 2 如何将 cpu 上面的数据放到 GPU 上去？\n",
    "# 3 一般情况下1， 2都是一起完成的\n",
    "# 4 如何生成随机数并将其转化为GPU上的tensor的格式？\n",
    "\n",
    "# 1\n",
    "x_cpu_tensor = torch.from_numpy(x).float()\n",
    "y_cpu_tensor = torch.from_numpy(y).float()\n",
    "# print('x_cpu_tensor' + x_cpu_tensor.type())\n",
    "\n",
    "# 2\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu' \n",
    "# 这里由于我装好了GPU版本的torch就是gpu，所以 device = 'cuda'\n",
    "x_gpu_tensor = x_cpu_tensor.to(device)\n",
    "y_gpu_tensor = y_cpu_tensor.to(device)\n",
    "# print('x_gpu_tensor' + x_gpu_tensor.type())\n",
    "\n",
    "# 3\n",
    "# x_gpu_tensor = torch.from_numpy(x).float().ro(device)\n",
    "\n",
    "# 4\n",
    "a_cpu_tensor = torch.randn(1, requires_grad = True, dtype = torch.float)\n",
    "\n",
    "a_gpu_tensor_wrong = torch.randn(1, requires_grad = True, dtype = torch.float).to(device) #这会导致 grad 被“shadow”\n",
    "\n",
    "a_gpu_tensor_right_bad = torch.randn(1, dtype=torch.float).to(device)\n",
    "a_gpu_tensor_right_bad.requires_grad_()\n",
    "\n",
    "a_gpu_tensor_right_good = torch.randn(1, requires_grad = True, dtype = torch.float, device = device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a36cff76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MSELoss()"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 反向传播——以y = a + b * x 为例\n",
    "# 1 复杂的做法——自己计算\n",
    "# 2 简单的做法——使用optimizer（优化器），以sgd为例\n",
    "# 3 简单的做法——使用nn中封装的损失函数，以MSEloss为例\n",
    "# 4 常见的损失函数\n",
    "# 5 用类进行封装——以线性分类器为例\n",
    "# 6 用函数进行进一步封装——以线性分类器为例\n",
    "# 7 将输入数据用函数封装\n",
    "# 8 划分数据集\n",
    "\n",
    "\n",
    "# 反向传播之前要做的准备\n",
    "lr = 1e-1\n",
    "n_epochs = 1000\n",
    "a = torch.randn(1, requires_grad = True, dtype = torch.float, device = 'cuda')\n",
    "b = torch.randn(1, requires_grad = True, dtype = torch.float, device = 'cuda')\n",
    "\n",
    "x_train_tensor = x_gpu_tensor\n",
    "y_train_tensor = y_gpu_tensor\n",
    "# 1\n",
    "for epoch in range(n_epochs):\n",
    "    yhat = a + b * x_train_tensor\n",
    "    error = y_train_tensor - yhat\n",
    "    loss = (error ** 2).mean()\n",
    "\n",
    "    # a_grad = -2 * error.mean()\n",
    "    # b_grad = -2 * (x_tensor * error).mean()\n",
    "    # We just tell PyTorch to work its way BACKWARDS from the specified loss!\n",
    "    loss.backward() #和上面两句代码等价\n",
    "    \n",
    "    # print(a.grad)\n",
    "    \n",
    "    # What about UPDATING the parameters? Not so fast...\n",
    "    \n",
    "    # FIRST ATTEMPT——wrong\n",
    "    # AttributeError: 'NoneType' object has no attribute 'zero_'\n",
    "    # a = a - lr * a.grad\n",
    "    # print(a)\n",
    "\n",
    "    # SECOND ATTEMPT——wrong\n",
    "    # RuntimeError: a leaf Variable that requires grad has been used in an in-place operation.\n",
    "    # a -= lr * a.grad\n",
    "    \n",
    "    # THIRD ATTEMPT——right\n",
    "    # We need to use NO_GRAD to keep the update out of the gradient computation\n",
    "    # Why is that? It boils down to the DYNAMIC GRAPH that PyTorch uses...\n",
    "    with torch.no_grad():\n",
    "        a -= lr * a.grad\n",
    "    \n",
    "    # PyTorch is \"clingy\" to its computed gradients, we need to tell it to let it go...\n",
    "    a.grad.zero_() # 在这个迭代方法中这句代码是必不可少的，否则就会出现错误\n",
    "\n",
    "    \n",
    "# 2  \n",
    "optimizer = optim.SGD([a, b], lr=lr)  # SGD 随机梯度下降，\n",
    "# 但实际上我们经常使用像AdaGrad，RMSProp，Adam等等更为优秀的优化器来训练神经网络。\n",
    "for epoch in range(n_epochs):\n",
    "    yhat = a + b * x_train_tensor\n",
    "    error = y_train_tensor - yhat\n",
    "    loss = (error ** 2).mean()\n",
    "\n",
    "    loss.backward()\n",
    "    \n",
    "    #with torch.no_grad():\n",
    "    #    a -= lr * a.grad   \n",
    "    # 下面这句代码与上两句代码等价 \n",
    "    optimizer.step() \n",
    "    \n",
    "    # a.grad.zero_() \n",
    "    # 下面这句代码与上两句代码等价\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "# 3\n",
    "loss_fn = nn.MSELoss(reduction='mean')\n",
    "optimizer = optim.SGD([a, b], lr=lr)\n",
    "for epoch in range(n_epochs):\n",
    "    yhat = a + b * x_train_tensor\n",
    "    \n",
    "    # error = y_tensor - yhat\n",
    "    # loss = (error ** 2).mean()\n",
    "    # 下一行代码和上两行代码等价\n",
    "    loss = loss_fn(y_train_tensor, yhat)\n",
    "\n",
    "    loss.backward()    \n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "# 4\n",
    "# 基本用法\n",
    "# criterion = LossCriterion() #构造函数有自己的参数\n",
    "# loss = criterion(x, y) #调用标准时也有参数\n",
    "\n",
    "# 4 - 1 L1范数损失 L1Loss\n",
    "torch.nn.L1Loss(reduction='mean')\n",
    "\n",
    "# 4 - 2 均方误差损失 MSELoss\n",
    "torch.nn.MSELoss(reduction='mean')\n",
    "\n",
    "# 详见 https://blog.csdn.net/shanglianlm/article/details/85019768"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f70b2fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5\n",
    "class ManualLinearRegression(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # To make \"a\" and \"b\" real parameters of the model, we need to wrap them with nn.Parameter\n",
    "        self.a = nn.Parameter(torch.randn(1, requires_grad=True, dtype=torch.float))\n",
    "        self.b = nn.Parameter(torch.randn(1, requires_grad=True, dtype=torch.float))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Computes the outputs / predictions\n",
    "        return self.a + self.b * x\n",
    "\n",
    "model = ManualLinearRegression().to(device)\n",
    "loss_fn = nn.MSELoss(reduction='mean')\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # What is this?!?\n",
    "    model.train()\n",
    "\n",
    "    # No more manual prediction!\n",
    "    # yhat = a + b * x_tensor\n",
    "    yhat = model(x_train_tensor)\n",
    "    \n",
    "    loss = loss_fn(y_train_tensor, yhat)\n",
    "    loss.backward()    \n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d2e7e0fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6\n",
    "class LayerLinearRegression(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Instead of our custom parameters, we use a Linear layer with single input and single output\n",
    "        self.linear = nn.Linear(1, 1)\n",
    "                \n",
    "    def forward(self, x):\n",
    "        # Now it only takes a call to the layer to make predictions\n",
    "        return self.linear(x)\n",
    "\n",
    "# Alternatively, you can use a Sequential model\n",
    "model = nn.Sequential(nn.Linear(1, 1)).to(device)\n",
    "\n",
    "def make_train_step(model, loss_fn, optimizer):\n",
    "    # Builds function that performs a step in the train loop\n",
    "    def train_step(x, y):\n",
    "        # Sets model to TRAIN mode\n",
    "        model.train()\n",
    "        # Makes predictions\n",
    "        yhat = model(x)\n",
    "        # Computes loss\n",
    "        loss = loss_fn(y, yhat)\n",
    "        # Computes gradients\n",
    "        loss.backward()\n",
    "        # Updates parameters and zeroes gradients\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        # Returns the loss\n",
    "        return loss.item()\n",
    "    \n",
    "    # Returns the function that will be called inside the train loop\n",
    "    return train_step\n",
    "\n",
    "# Creates the train_step function for our model, loss function and optimizer\n",
    "train_step = make_train_step(model, loss_fn, optimizer)\n",
    "losses = []\n",
    "\n",
    "# For each epoch...\n",
    "for epoch in range(n_epochs):\n",
    "    # Performs one train step and returns the corresponding loss\n",
    "    loss = train_step(x_train_tensor, y_train_tensor)\n",
    "    losses.append(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "935f4308",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7\n",
    "from torch.utils.data import Dataset, TensorDataset\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, x_tensor, y_tensor):\n",
    "        self.x = x_tensor\n",
    "        self.y = y_tensor\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return (self.x[index], self.y[index])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "x_train = x\n",
    "y_train = y\n",
    "# Wait, is this a CPU tensor now? Why? Where is .to(device)?\n",
    "x_train_tensor = torch.from_numpy(x_train).float()\n",
    "y_train_tensor = torch.from_numpy(y_train).float()\n",
    "\n",
    "train_data = CustomDataset(x_train_tensor, y_train_tensor)\n",
    "train_data = TensorDataset(x_train_tensor, y_train_tensor)  # 二者等价，直接用TensorDateset即可\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "train_loader = DataLoader(dataset = train_data, batch_size = 16, shuffle = True)\n",
    "# next(iter(train_loader)) # 每16个数据为一组，这行代码是为了查看下一组\n",
    "\n",
    "losses = []\n",
    "train_step = make_train_step(model, loss_fn, optimizer)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    for x_batch, y_batch in train_loader:\n",
    "        # the dataset \"lives\" in the CPU, so do our mini-batches\n",
    "        # therefore, we need to send those mini-batches to the\n",
    "        # device where the model \"lives\"\n",
    "        # 大概意思就是说上面的model是位于device上的，所以要训练的数据也要to（device）\n",
    "        x_batch = x_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "        \n",
    "        loss = train_step(x_batch, y_batch)\n",
    "        losses.append(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "97696b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8\n",
    "from torch.utils.data.dataset import random_split\n",
    "from torch.utils.data import Dataset, TensorDataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "x_tensor = torch.from_numpy(x).float()\n",
    "y_tensor = torch.from_numpy(y).float()\n",
    "\n",
    "a = torch.randn(1, requires_grad = True, dtype = torch.float, device = device)\n",
    "b = torch.randn(1, requires_grad = True, dtype = torch.float, device = device)\n",
    "\n",
    "dataset = TensorDataset(x_tensor, y_tensor)\n",
    "\n",
    "train_dataset, val_dataset = random_split(dataset, [80, 20]) # 将数据集8、2分\n",
    "\n",
    "train_loader = DataLoader(dataset = train_dataset, batch_size = 16)\n",
    "val_loader = DataLoader(dataset = val_dataset, batch_size = 20)\n",
    "\n",
    "losses = []\n",
    "val_losses = []\n",
    "\n",
    "lr = 0.01\n",
    "loss_fn = nn.MSELoss(reduction='mean')\n",
    "optimizer = optim.SGD([a, b], lr=lr)\n",
    "train_step = make_train_step(model, loss_fn, optimizer)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    for x_batch, y_batch in train_loader:\n",
    "        x_batch = x_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "\n",
    "        loss = train_step(x_batch, y_batch)\n",
    "        losses.append(loss)\n",
    "        \n",
    "    with torch.no_grad():\n",
    "        for x_val, y_val in val_loader:\n",
    "            x_val = x_val.to(device)\n",
    "            y_val = y_val.to(device)\n",
    "            \n",
    "            model.eval()\n",
    "\n",
    "            yhat = model(x_val)\n",
    "            val_loss = loss_fn(y_val, yhat)\n",
    "            val_losses.append(val_loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6553933a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('0.weight', tensor([[-0.6978]], device='cuda:0')),\n",
       "             ('0.bias', tensor([0.6750], device='cuda:0'))])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 检查模型的参数\n",
    "model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "08eaf3ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 708.8596801757812\n",
      "1 690.9149780273438\n",
      "2 673.413330078125\n",
      "3 656.3956909179688\n",
      "4 639.882080078125\n",
      "5 623.9313354492188\n",
      "6 608.3696899414062\n",
      "7 593.2067260742188\n",
      "8 578.5216674804688\n",
      "9 564.3375244140625\n",
      "10 550.6903076171875\n",
      "11 537.5280151367188\n",
      "12 524.7810668945312\n",
      "13 512.5166625976562\n",
      "14 500.6340637207031\n",
      "15 489.08966064453125\n",
      "16 477.79022216796875\n",
      "17 466.77532958984375\n",
      "18 456.053466796875\n",
      "19 445.64227294921875\n",
      "20 435.55413818359375\n",
      "21 425.7245788574219\n",
      "22 416.1257629394531\n",
      "23 406.7690124511719\n",
      "24 397.6847229003906\n",
      "25 388.84490966796875\n",
      "26 380.1683349609375\n",
      "27 371.7011413574219\n",
      "28 363.4910888671875\n",
      "29 355.51470947265625\n",
      "30 347.69268798828125\n",
      "31 340.01910400390625\n",
      "32 332.483642578125\n",
      "33 325.1187744140625\n",
      "34 317.9007873535156\n",
      "35 310.8634033203125\n",
      "36 303.9915771484375\n",
      "37 297.2645263671875\n",
      "38 290.67572021484375\n",
      "39 284.2334899902344\n",
      "40 277.91925048828125\n",
      "41 271.7469787597656\n",
      "42 265.7047424316406\n",
      "43 259.7878723144531\n",
      "44 253.98158264160156\n",
      "45 248.2821044921875\n",
      "46 242.69065856933594\n",
      "47 237.20904541015625\n",
      "48 231.8339080810547\n",
      "49 226.56724548339844\n",
      "50 221.40257263183594\n",
      "51 216.33059692382812\n",
      "52 211.3619384765625\n",
      "53 206.47537231445312\n",
      "54 201.69815063476562\n",
      "55 197.0139617919922\n",
      "56 192.42262268066406\n",
      "57 187.9241485595703\n",
      "58 183.52015686035156\n",
      "59 179.2158966064453\n",
      "60 174.98707580566406\n",
      "61 170.82998657226562\n",
      "62 166.75291442871094\n",
      "63 162.7537841796875\n",
      "64 158.8396453857422\n",
      "65 155.0015106201172\n",
      "66 151.23516845703125\n",
      "67 147.53173828125\n",
      "68 143.9004364013672\n",
      "69 140.32989501953125\n",
      "70 136.83367919921875\n",
      "71 133.40090942382812\n",
      "72 130.03076171875\n",
      "73 126.7263412475586\n",
      "74 123.48867797851562\n",
      "75 120.30767822265625\n",
      "76 117.19444274902344\n",
      "77 114.1406478881836\n",
      "78 111.1410903930664\n",
      "79 108.20095825195312\n",
      "80 105.31700134277344\n",
      "81 102.48847198486328\n",
      "82 99.71903991699219\n",
      "83 97.00950622558594\n",
      "84 94.35234069824219\n",
      "85 91.75630187988281\n",
      "86 89.2152099609375\n",
      "87 86.7196273803711\n",
      "88 84.27461242675781\n",
      "89 81.88554382324219\n",
      "90 79.55389404296875\n",
      "91 77.27125549316406\n",
      "92 75.03131103515625\n",
      "93 72.83759307861328\n",
      "94 70.69365692138672\n",
      "95 68.60230255126953\n",
      "96 66.55718994140625\n",
      "97 64.55545806884766\n",
      "98 62.602596282958984\n",
      "99 60.69502639770508\n",
      "100 58.834564208984375\n",
      "101 57.01416778564453\n",
      "102 55.2413330078125\n",
      "103 53.505672454833984\n",
      "104 51.81746292114258\n",
      "105 50.1662483215332\n",
      "106 48.55632400512695\n",
      "107 46.99039840698242\n",
      "108 45.46366882324219\n",
      "109 43.97895431518555\n",
      "110 42.533485412597656\n",
      "111 41.127906799316406\n",
      "112 39.756351470947266\n",
      "113 38.42202377319336\n",
      "114 37.12771987915039\n",
      "115 35.86724853515625\n",
      "116 34.64126968383789\n",
      "117 33.452476501464844\n",
      "118 32.29436111450195\n",
      "119 31.172412872314453\n",
      "120 30.082218170166016\n",
      "121 29.024105072021484\n",
      "122 27.998783111572266\n",
      "123 27.003009796142578\n",
      "124 26.0360107421875\n",
      "125 25.0992374420166\n",
      "126 24.192283630371094\n",
      "127 23.31170654296875\n",
      "128 22.45963478088379\n",
      "129 21.6323184967041\n",
      "130 20.831134796142578\n",
      "131 20.055002212524414\n",
      "132 19.302934646606445\n",
      "133 18.57688331604004\n",
      "134 17.872562408447266\n",
      "135 17.191707611083984\n",
      "136 16.532146453857422\n",
      "137 15.894706726074219\n",
      "138 15.278345108032227\n",
      "139 14.682326316833496\n",
      "140 14.106388092041016\n",
      "141 13.550167083740234\n",
      "142 13.011679649353027\n",
      "143 12.49349308013916\n",
      "144 11.992561340332031\n",
      "145 11.509183883666992\n",
      "146 11.042516708374023\n",
      "147 10.592363357543945\n",
      "148 10.15839672088623\n",
      "149 9.73976993560791\n",
      "150 9.336116790771484\n",
      "151 8.947623252868652\n",
      "152 8.573763847351074\n",
      "153 8.213275909423828\n",
      "154 7.866574287414551\n",
      "155 7.5329461097717285\n",
      "156 7.211915493011475\n",
      "157 6.902961730957031\n",
      "158 6.606178283691406\n",
      "159 6.3209710121154785\n",
      "160 6.04677677154541\n",
      "161 5.783336639404297\n",
      "162 5.530277252197266\n",
      "163 5.286866188049316\n",
      "164 5.053454875946045\n",
      "165 4.829583644866943\n",
      "166 4.614582061767578\n",
      "167 4.40815544128418\n",
      "168 4.210363388061523\n",
      "169 4.0207719802856445\n",
      "170 3.838834762573242\n",
      "171 3.664463996887207\n",
      "172 3.497368574142456\n",
      "173 3.3372509479522705\n",
      "174 3.1839866638183594\n",
      "175 3.0373380184173584\n",
      "176 2.896843194961548\n",
      "177 2.7623724937438965\n",
      "178 2.633854389190674\n",
      "179 2.5107340812683105\n",
      "180 2.393092155456543\n",
      "181 2.2805917263031006\n",
      "182 2.1730363368988037\n",
      "183 2.070234775543213\n",
      "184 1.9720288515090942\n",
      "185 1.878189206123352\n",
      "186 1.7885420322418213\n",
      "187 1.7029105424880981\n",
      "188 1.621372938156128\n",
      "189 1.5432535409927368\n",
      "190 1.4687999486923218\n",
      "191 1.3979933261871338\n",
      "192 1.3302193880081177\n",
      "193 1.2656623125076294\n",
      "194 1.2041434049606323\n",
      "195 1.1454603672027588\n",
      "196 1.0895754098892212\n",
      "197 1.0362539291381836\n",
      "198 0.985459566116333\n",
      "199 0.937055230140686\n",
      "200 0.8910313248634338\n",
      "201 0.8471365571022034\n",
      "202 0.805336594581604\n",
      "203 0.7655866742134094\n",
      "204 0.7277588844299316\n",
      "205 0.6917570233345032\n",
      "206 0.6575353741645813\n",
      "207 0.6249678134918213\n",
      "208 0.5940061211585999\n",
      "209 0.5645290613174438\n",
      "210 0.5365279912948608\n",
      "211 0.5099056363105774\n",
      "212 0.4846009910106659\n",
      "213 0.46058815717697144\n",
      "214 0.4377268850803375\n",
      "215 0.41603541374206543\n",
      "216 0.39541324973106384\n",
      "217 0.37578797340393066\n",
      "218 0.35716840624809265\n",
      "219 0.33947890996932983\n",
      "220 0.3226510286331177\n",
      "221 0.3066847324371338\n",
      "222 0.291509747505188\n",
      "223 0.2771083116531372\n",
      "224 0.26341503858566284\n",
      "225 0.2504243552684784\n",
      "226 0.238066166639328\n",
      "227 0.22634729743003845\n",
      "228 0.2152010202407837\n",
      "229 0.20463493466377258\n",
      "230 0.19459255039691925\n",
      "231 0.1850498467683792\n",
      "232 0.1759917438030243\n",
      "233 0.16738596558570862\n",
      "234 0.1592162549495697\n",
      "235 0.15144780278205872\n",
      "236 0.1440720409154892\n",
      "237 0.1370704174041748\n",
      "238 0.13041146099567413\n",
      "239 0.12409524619579315\n",
      "240 0.11808808147907257\n",
      "241 0.11238380521535873\n",
      "242 0.1069621592760086\n",
      "243 0.10181759297847748\n",
      "244 0.09693498909473419\n",
      "245 0.09229320287704468\n",
      "246 0.08787935972213745\n",
      "247 0.08368563652038574\n",
      "248 0.07969984412193298\n",
      "249 0.07591120153665543\n",
      "250 0.07230587303638458\n",
      "251 0.0688788965344429\n",
      "252 0.06562139093875885\n",
      "253 0.06252653151750565\n",
      "254 0.05957931652665138\n",
      "255 0.056779004633426666\n",
      "256 0.05411588400602341\n",
      "257 0.05158230662345886\n",
      "258 0.049171045422554016\n",
      "259 0.0468774251639843\n",
      "260 0.044694170355796814\n",
      "261 0.042616669088602066\n",
      "262 0.040640827268362045\n",
      "263 0.038758039474487305\n",
      "264 0.03696651756763458\n",
      "265 0.03525998443365097\n",
      "266 0.03363470733165741\n",
      "267 0.03208649158477783\n",
      "268 0.030613448470830917\n",
      "269 0.02920917421579361\n",
      "270 0.02787162736058235\n",
      "271 0.026596831157803535\n",
      "272 0.025382492691278458\n",
      "273 0.02422568015754223\n",
      "274 0.023122938349843025\n",
      "275 0.022071827203035355\n",
      "276 0.021070200949907303\n",
      "277 0.02011505886912346\n",
      "278 0.01920424774289131\n",
      "279 0.018336039036512375\n",
      "280 0.01750892587006092\n",
      "281 0.016718879342079163\n",
      "282 0.015965763479471207\n",
      "283 0.0152481310069561\n",
      "284 0.014564009383320808\n",
      "285 0.013911125250160694\n",
      "286 0.013288304209709167\n",
      "287 0.012694044038653374\n",
      "288 0.012126702815294266\n",
      "289 0.011585481464862823\n",
      "290 0.01106882095336914\n",
      "291 0.010575451888144016\n",
      "292 0.010104917921125889\n",
      "293 0.009655363857746124\n",
      "294 0.00922595988959074\n",
      "295 0.008816102519631386\n",
      "296 0.008424678817391396\n",
      "297 0.00805118028074503\n",
      "298 0.007693763822317123\n",
      "299 0.007352799642831087\n",
      "300 0.007026936858892441\n",
      "301 0.00671608280390501\n",
      "302 0.006418420933187008\n",
      "303 0.006134556140750647\n",
      "304 0.0058628832921385765\n",
      "305 0.005603840108960867\n",
      "306 0.005355855915695429\n",
      "307 0.005119030363857746\n",
      "308 0.004892731085419655\n",
      "309 0.004676536191254854\n",
      "310 0.00446975976228714\n",
      "311 0.004272157326340675\n",
      "312 0.00408326368778944\n",
      "313 0.0039027573075145483\n",
      "314 0.003730141557753086\n",
      "315 0.0035655833780765533\n",
      "316 0.0034074466675519943\n",
      "317 0.0032566676381975412\n",
      "318 0.003112546633929014\n",
      "319 0.002974855247884989\n",
      "320 0.0028430121019482613\n",
      "321 0.0027170784305781126\n",
      "322 0.0025966642424464226\n",
      "323 0.0024816328659653664\n",
      "324 0.0023714762646704912\n",
      "325 0.0022662444971501827\n",
      "326 0.002165638841688633\n",
      "327 0.0020694113336503506\n",
      "328 0.001977446023374796\n",
      "329 0.001889490056782961\n",
      "330 0.0018053974490612745\n",
      "331 0.00172499381005764\n",
      "332 0.001648150384426117\n",
      "333 0.0015746576245874166\n",
      "334 0.0015044016763567924\n",
      "335 0.001437199767678976\n",
      "336 0.0013729967176914215\n",
      "337 0.0013116109184920788\n",
      "338 0.0012529060477390885\n",
      "339 0.0011968542821705341\n",
      "340 0.0011431809980422258\n",
      "341 0.0010919407941401005\n",
      "342 0.0010428958339616656\n",
      "343 0.0009960520546883345\n",
      "344 0.0009512794204056263\n",
      "345 0.0009084788616746664\n",
      "346 0.0008675438584759831\n",
      "347 0.0008284216746687889\n",
      "348 0.0007910497952252626\n",
      "349 0.0007553048199042678\n",
      "350 0.0007211361080408096\n",
      "351 0.0006884969188831747\n",
      "352 0.0006573035498149693\n",
      "353 0.000627477013040334\n",
      "354 0.0005989780183881521\n",
      "355 0.0005717467865906656\n",
      "356 0.0005457192310132086\n",
      "357 0.0005208476795814931\n",
      "358 0.0004970973241142929\n",
      "359 0.00047439237823709846\n",
      "360 0.0004527021083049476\n",
      "361 0.00043198111234232783\n",
      "362 0.00041218334808945656\n",
      "363 0.00039326760452240705\n",
      "364 0.00037519983015954494\n",
      "365 0.0003579548210836947\n",
      "366 0.0003414546954445541\n",
      "367 0.0003257091739214957\n",
      "368 0.0003106704098172486\n",
      "369 0.0002963041188195348\n",
      "370 0.00028258422389626503\n",
      "371 0.0002694881986826658\n",
      "372 0.00025698135141283274\n",
      "373 0.00024504386237822473\n",
      "374 0.0002336361212655902\n",
      "375 0.00022273807553574443\n",
      "376 0.00021235276653897017\n",
      "377 0.0002024264249484986\n",
      "378 0.0001929527788888663\n",
      "379 0.00018391643243376166\n",
      "380 0.0001752846728777513\n",
      "381 0.00016704955487512052\n",
      "382 0.00015919639554340392\n",
      "383 0.00015169690595939755\n",
      "384 0.0001445422531105578\n",
      "385 0.00013772085367236286\n",
      "386 0.0001312063541263342\n",
      "387 0.00012499364675022662\n",
      "388 0.00011906572035513818\n",
      "389 0.00011341218487359583\n",
      "390 0.00010801681492011994\n",
      "391 0.00010287368786521256\n",
      "392 9.796681842999533e-05\n",
      "393 9.328902524430305e-05\n",
      "394 8.882765541784465e-05\n",
      "395 8.456904470222071e-05\n",
      "396 8.051491749938577e-05\n",
      "397 7.664820441277698e-05\n",
      "398 7.295895193237811e-05\n",
      "399 6.944527558516711e-05\n",
      "400 6.609387492062524e-05\n",
      "401 6.289989687502384e-05\n",
      "402 5.9857065934920684e-05\n",
      "403 5.69543190067634e-05\n",
      "404 5.419296212494373e-05\n",
      "405 5.1556486141635105e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "406 4.9047142965719104e-05\n",
      "407 4.665308370022103e-05\n",
      "408 4.437381721800193e-05\n",
      "409 4.220255505060777e-05\n",
      "410 4.013648140244186e-05\n",
      "411 3.816717071458697e-05\n",
      "412 3.6289980926085263e-05\n",
      "413 3.45068474416621e-05\n",
      "414 3.2802516216179356e-05\n",
      "415 3.1182626116788015e-05\n",
      "416 2.9641299988725223e-05\n",
      "417 2.8172891688882373e-05\n",
      "418 2.6775383958010934e-05\n",
      "419 2.5444589482503943e-05\n",
      "420 2.4179003958124667e-05\n",
      "421 2.2971435100771487e-05\n",
      "422 2.1827692762599327e-05\n",
      "423 2.073553332593292e-05\n",
      "424 1.96964101633057e-05\n",
      "425 1.870907362899743e-05\n",
      "426 1.776927092578262e-05\n",
      "427 1.687433905317448e-05\n",
      "428 1.6023977877921425e-05\n",
      "429 1.5215549865388311e-05\n",
      "430 1.4446908608078957e-05\n",
      "431 1.3715824934479315e-05\n",
      "432 1.3018914614804089e-05\n",
      "433 1.2358239473542199e-05\n",
      "434 1.1729356629075482e-05\n",
      "435 1.1131680366815999e-05\n",
      "436 1.0562827810645103e-05\n",
      "437 1.0023159120464697e-05\n",
      "438 9.509434676147066e-06\n",
      "439 9.022408448799979e-06\n",
      "440 8.55841426528059e-06\n",
      "441 8.118114237731788e-06\n",
      "442 7.699327397858724e-06\n",
      "443 7.301923687919043e-06\n",
      "444 6.9244642872945406e-06\n",
      "445 6.565488547494169e-06\n",
      "446 6.225061042641755e-06\n",
      "447 5.9011817938880995e-06\n",
      "448 5.594504727923777e-06\n",
      "449 5.302577847032808e-06\n",
      "450 5.0257312977919355e-06\n",
      "451 4.7628946049371734e-06\n",
      "452 4.513137355388608e-06\n",
      "453 4.276635991118383e-06\n",
      "454 4.051673386129551e-06\n",
      "455 3.838300472125411e-06\n",
      "456 3.635760094766738e-06\n",
      "457 3.444427647991688e-06\n",
      "458 3.262472546339268e-06\n",
      "459 3.0886267268215306e-06\n",
      "460 2.92475169771933e-06\n",
      "461 2.769658294710098e-06\n",
      "462 2.6218585844617337e-06\n",
      "463 2.4821865736157633e-06\n",
      "464 2.3495899768022355e-06\n",
      "465 2.224228865088662e-06\n",
      "466 2.104299483107752e-06\n",
      "467 1.9912590687454212e-06\n",
      "468 1.8846742477762746e-06\n",
      "469 1.7830255956141627e-06\n",
      "470 1.686541736489744e-06\n",
      "471 1.5953908132360084e-06\n",
      "472 1.5092790590642835e-06\n",
      "473 1.4272752650867915e-06\n",
      "474 1.3495819075615145e-06\n",
      "475 1.2761170182784554e-06\n",
      "476 1.2068027217537747e-06\n",
      "477 1.1405185205148882e-06\n",
      "478 1.0781088803923922e-06\n",
      "479 1.0192612762693898e-06\n",
      "480 9.631232842366444e-07\n",
      "481 9.101892146645696e-07\n",
      "482 8.600151204518625e-07\n",
      "483 8.128412218866288e-07\n",
      "484 7.678053179915878e-07\n",
      "485 7.253009925989318e-07\n",
      "486 6.85201939631952e-07\n",
      "487 6.472173481597565e-07\n",
      "488 6.114043458183005e-07\n",
      "489 5.769765039076447e-07\n",
      "490 5.450286266750481e-07\n",
      "491 5.143689350006753e-07\n",
      "492 4.854722419622703e-07\n",
      "493 4.581561938721279e-07\n",
      "494 4.3252819637018547e-07\n",
      "495 4.083300382262678e-07\n",
      "496 3.8519507938872266e-07\n",
      "497 3.6347347531773266e-07\n",
      "498 3.427775823183765e-07\n",
      "499 3.233214442843746e-07\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    " \n",
    "# N is batch size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    " \n",
    "# Create random Tensors to hold inputs and outputs.\n",
    "x = torch.randn(N, D_in)\n",
    "y = torch.randn(N, D_out)\n",
    " \n",
    "# Use the nn package to define our model and loss function.\n",
    "model = torch.nn.Sequential(\n",
    "          torch.nn.Linear(D_in, H),\n",
    "          torch.nn.ReLU(),\n",
    "          torch.nn.Linear(H, D_out),\n",
    "        )\n",
    "loss_fn = torch.nn.MSELoss(reduction = 'sum')\n",
    " \n",
    "# Use the optim package to define an Optimizer that will update the weights of\n",
    "# the model for us. Here we will use Adam; the optim package contains many other\n",
    "# optimization algoriths. The first argument to the Adam constructor tells the\n",
    "# optimizer which Tensors it should update.\n",
    "learning_rate = 1e-4\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "for t in range(500):\n",
    "  # Forward pass: compute predicted y by passing x to the model.\n",
    "  y_pred = model(x)\n",
    " \n",
    "  # Compute and print loss.\n",
    "  loss = loss_fn(y_pred, y)\n",
    "  print(t, loss.item())\n",
    "  \n",
    "  # Before the backward pass, use the optimizer object to zero all of the\n",
    "  # gradients for the Tensors it will update (which are the learnable weights\n",
    "  # of the model)\n",
    "  optimizer.zero_grad()\n",
    " \n",
    "  # Backward pass: compute gradient of the loss with respect to model parameters\n",
    "  loss.backward()\n",
    " \n",
    "  # Calling the step function on an Optimizer makes an update to its parameters\n",
    "  optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "debd8c1f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
